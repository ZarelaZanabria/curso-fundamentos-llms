{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyO0o8eGNLo3fl92KXN7scQX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"RSXiOUrucBSi","executionInfo":{"status":"ok","timestamp":1738430185691,"user_tz":300,"elapsed":19308,"user":{"displayName":"Jhenner Tigreros","userId":"10282691994518148206"}},"outputId":"3d7f7e83-6a40-46a4-869c-a5ac8bcb5031"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.0)\n","Collecting xformers\n","  Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n","Collecting np\n","  Downloading np-1.0.2.tar.gz (7.4 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xformers) (1.26.4)\n","Collecting torch==2.6.0 (from xformers)\n","  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->xformers)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->xformers)\n","  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->xformers)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.2.0 (from torch==2.6.0->xformers)\n","  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->xformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->xformers) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->xformers) (3.0.2)\n","Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl (44.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n","\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m761.0/766.7 MB\u001b[0m \u001b[31m142.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m^C\n"]}],"source":["!pip install einops xformers np"]},{"cell_type":"code","source":["import np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.modules import ModuleList\n","from torch.nn.modules.normalization import LayerNorm\n","from torch import nn\n","\n","import copy\n","import math"],"metadata":{"id":"wJjFQShadGTx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"gu3RiwkfeXJU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mfXyaTFSoVI7","executionInfo":{"status":"ok","timestamp":1738365035781,"user_tz":300,"elapsed":3,"user":{"displayName":"Jhenner Tigreros","userId":"10282691994518148206"}},"outputId":"fc565b2b-181d-48d9-e501-0df3eba6a7c1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["def _get_clones(module, n):\n","    return ModuleList([copy.deepcopy(module) for i in range(n)])"],"metadata":{"id":"FlUvCj4dfE1y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Conv1D(nn.Module):\n","    def __init__(self, nx, nf):\n","        '''\n","        nx: Numero de datos de entrada.\n","        nf: Numero de filtros. (Canales de salida).\n","        '''\n","        super().__init__()\n","        self.nf = nf\n","        #Inicializando una matriz vacia de pesos del tamaño (nx)X(nf)\n","        w = torch.empty(nx, nf)\n","        #Inicializando los pesos con una distribución normal.\n","        nn.init.normal_(w, std=0.02)\n","        #Calculando los pesos y sesgos encodeandos usando nn.Parameter\n","        self.weight = nn.Parameter(w)\n","        self.bias = nn.Parameter(torch.zeros(nf))\n","\n","    def forward(self, x):\n","        '''x:Tensor de entrada.'''\n","        #El tamaño de la salida es la suna de la segunda dimensión de X y el número de filtros nf.\n","        size_out = x.size()[:-1] + (self.nf,)\n","        # Producot punto Q,K(Transpuesta) y V\n","        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)# x.view ayuda a calcular la transpuesta.\n","        x = x.view(*size_out)\n","        return x"],"metadata":{"id":"Kfp70gl_fHoL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, dropout, d_model=768, nx=768*4):\n","        super().__init__()\n","        self.c_fc    = Conv1D(d_model, nx)\n","        self.c_proj  = Conv1D(nx, d_model)\n","        self.act     = F.gelu\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.dropout(self.c_proj(self.act(self.c_fc(x))))"],"metadata":{"id":"Xms1IIJUfO8j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Attention(nn.Module):\n","    def __init__(self, d_model=768, n_head=12, n_ctx=1024, d_head=64, bias=True, scale=False):\n","        '''Función de construcción\n","        Params:\n","        d_model:Dimensión que necesita ser ingresada en el modelo.\n","        n_head:La cantidad de heads de atención.\n","        n_ctx:Buffer para guardar los registros del sesgo.\n","        d_head:Dimesión de salida para el head.\n","        bias:Un booleano para saber si incluir el sesgo.\n","        scale: Escalar y estabilidad númerica (sqrt(dk))\n","        '''\n","        super().__init__()\n","        self.n_head  = n_head\n","        self.d_model = d_model\n","        self.c_attn  = Conv1D(d_model, d_model*3)\n","        self.scale   = scale\n","        self.softmax = nn.Softmax(dim=-1)\n","        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n","        self.dropout = nn.Dropout(0.1)\n","        self.c_proj  = Conv1D(d_model, d_model)\n","\n","    def split_heads(self, x):\n","        \"\"\"\n","        Diviendo en la cantidad de heads y retornando.\n","        return shape [`batch`, `head`, `sequence`, `features`]\n","        \"\"\"\n","        new_shape = x.size()[:-1] + (self.n_head, x.size(-1)//self.n_head)\n","        x = x.view(*new_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def _attn(self, q, k, v, attn_mask=None):\n","        \"\"\"Función de antención principal.\n","        Que calcula usando la formula de producto punto de atención.\"\"\"\n","        scores  = torch.matmul(q, k.transpose(-2, -1))# producto punto de Q*K(t)\n","        if self.scale: scores = scores/math.sqrt(v.size(-1))# escalandola por sqrt(dk)\n","        nd, ns  = scores.size(-2), scores.size(-1)\n","        if attn_mask is not None: scores = scores + attn_mask# agregando los valores con la mascara de atención.\n","        scores  = self.softmax(scores)# añadiendo los valores de softmax\n","        scores  = self.dropout(scores) # función de dropout 0.1\n","        outputs = torch.matmul(scores, v) # Multiplicación final del puntaje por V.\n","        return outputs\n","\n","    def merge_heads(self, x):\n","        # Combinando todas las heads en una sola.\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        new_shape = x.size()[:-2] + (x.size(-2)*x.size(-1),)\n","        return x.view(*new_shape)\n","\n","    def forward(self, x):\n","        '''Función de para calcular atención, separar las heads y combinarlas de nuevo.'''\n","        x        = self.c_attn(x) #new `x` shape - `[1,3,2304]`\n","        q, k, v  = x.split(self.d_model, dim=2)\n","        q, k, v  = self.split_heads(q), self.split_heads(k), self.split_heads(v)\n","        out      = self._attn(q, k, v)\n","        out      = self.merge_heads(out)\n","        out      = self.c_proj(out)\n","        return out"],"metadata":{"id":"uravxQVpfZ4S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerBlock(nn.Module):\n","    def __init__(self, d_model=768, n_head=12, dropout=0.1):\n","        super(TransformerBlock, self).__init__()\n","        self.attn        = Attention(d_model=768, n_head=12, d_head=64, n_ctx=1024, bias=True, scale=False)\n","        self.feedforward = FeedForward(dropout=0.1, d_model=768, nx=768*4)\n","        self.ln_1        = LayerNorm(d_model)\n","        self.ln_2        = LayerNorm(d_model)\n","\n","    def forward(self, x):\n","        x = x + self.attn(self.ln_1(x))\n","        x = x + self.feedforward(self.ln_2(x))\n","        return x"],"metadata":{"id":"6tSwyE6QfiLP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GPT2(nn.Module):\n","    def __init__(self, nlayers=12, n_ctx=1024, d_model=768, vcb_sz=50257):\n","        '''nlayer: La cantidad de veces que queremos multiplicar el Transformer.\n","        n_ctx: El contexto, la cantidad total de tokens que puede ver en el pasado de las palabras.\n","        d_model:Dimesionos del modelo.\n","        vcb_sz:El tamaño del vocabulario usado en el entrenamiento.'''\n","        super(GPT2, self).__init__()\n","        self.nlayers = nlayers\n","        block        = TransformerBlock(d_model=768, n_head=12, dropout=0.1)\n","        self.h       = _get_clones(block, 12)\n","        self.wte     = nn.Embedding(vcb_sz, d_model)\n","        self.wpe     = nn.Embedding(n_ctx, d_model)\n","        self.drop    = nn.Dropout(0.1)\n","        self.ln_f    = LayerNorm(d_model)\n","        self.out     = nn.Linear(d_model, vcb_sz, bias=False)\n","        self.loss_fn = nn.CrossEntropyLoss()\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        '''Inicialización de los pesos.'''\n","        self.out.weight = self.wte.weight\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, module):\n","        '''Inicialización con la media y S.D.'''\n","        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n","            module.weight.data.normal_(mean=0.0, std=0.02)\n","            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n","                '''Data Bias zero'''\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    def forward(self, src, labels=None, pos_ids=None):\n","        '''Añadir el embedding posicional, dropping y añadiendo los inputs\n","           usados por la función de perdida y finalmente añadiendo la salida y la\n","           perdida.'''\n","        if pos_ids is None:\n","            pos_ids = torch.arange(0, src.size(-1)).unsqueeze(0)\n","        pos_ids = pos_ids.to(src.device)  # Asegurarse que los pos_ids están en el mismo device.\n","        inp = self.drop((self.wte(src) + self.wpe(pos_ids)))\n","        for i in range(self.nlayers): inp = self.h[i](inp)\n","        inp     = self.ln_f(inp)\n","        logits  = self.out(inp)\n","        outputs = (logits,) + (inp,)\n","\n","        if labels is not None:\n","            shift_logits = logits[..., :-1, :].contiguous()\n","            shift_labels = labels[..., 1:].contiguous()\n","            loss = self.loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n","            outputs = (loss,) + outputs\n","            return loss.mean()\n","        return logits"],"metadata":{"id":"Cm2dD9KyflYi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","import time\n","from transformers import GPT2Tokenizer"],"metadata":{"id":"Q1mPKZxdftI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2()"],"metadata":{"id":"ZuoC4itMfxqz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!curl --output gpt2-pytorch_model.bin https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmKHsWiHf7e9","executionInfo":{"status":"ok","timestamp":1738362900585,"user_tz":300,"elapsed":69534,"user":{"displayName":"Jhenner Tigreros","userId":"10282691994518148206"}},"outputId":"a8d915ff-bdd1-47e9-8cb3-0ef8d4787e99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  522M  100  522M    0     0  7761k      0  0:01:08  0:01:08 --:--:-- 13.4M\n"]}]},{"cell_type":"code","source":["model_dict = model.state_dict()\n","state_dict = torch.load(\"./gpt2-pytorch_model.bin\")\n","\n","old_keys = []\n","new_keys = []\n","for key in state_dict.keys():\n","    if \"mlp\" in key: #El diccionario de estado para el MLP feedforward debe ser cambiado por mlp\n","        new_key = key.replace(\"mlp\", \"feedforward\")\n","        new_keys.append(new_key)\n","        old_keys.append(key)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bqw4UaZKgLrm","executionInfo":{"status":"ok","timestamp":1738366651100,"user_tz":300,"elapsed":779,"user":{"displayName":"Jhenner Tigreros","userId":"10282691994518148206"}},"outputId":"03e37066-ef69-4693-b3b1-f8171a1a6853"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-51-0d65074896ff>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  state_dict = torch.load(\"./gpt2-pytorch_model.bin\")\n"]}]},{"cell_type":"code","source":["for old_key, new_key in zip(old_keys, new_keys):\n","    state_dict[new_key]=state_dict.pop(old_key)"],"metadata":{"id":"3r3EXVWNgV5A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict}\n","\n","model_dict.update(pretrained_dict)\n","model.load_state_dict(model_dict)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Z_H4pVIgX3D","executionInfo":{"status":"ok","timestamp":1738366668375,"user_tz":300,"elapsed":371,"user":{"displayName":"Jhenner Tigreros","userId":"10282691994518148206"}},"outputId":"7ddc19a2-4b43-4c90-c405-7dd7e2e6a4ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2(\n","  (h): ModuleList(\n","    (0-11): 12 x TransformerBlock(\n","      (attn): Attention(\n","        (c_attn): Conv1D()\n","        (softmax): Softmax(dim=-1)\n","        (dropout): Dropout(p=0.1, inplace=False)\n","        (c_proj): Conv1D()\n","      )\n","      (feedforward): FeedForward(\n","        (c_fc): Conv1D()\n","        (c_proj): Conv1D()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (wte): Embedding(50257, 768)\n","  (wpe): Embedding(1024, 768)\n","  (drop): Dropout(p=0.1, inplace=False)\n","  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","  (out): Linear(in_features=768, out_features=50257, bias=False)\n","  (loss_fn): CrossEntropyLoss()\n",")"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["total_params = sum(p.numel() for p in model.parameters())"],"metadata":{"id":"YllyrazTgeG8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["size_bytes = total_params * 4\n","size_mb = size_bytes / (1024 ** 2)\n","\n","print(f\"El tamaño total de GPT2 sin alteraciones es: {size_bytes} bytes o {size_mb:.2f} MB\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7DZCeOM8gf8h","executionInfo":{"status":"ok","timestamp":1738366723027,"user_tz":300,"elapsed":372,"user":{"displayName":"Jhenner Tigreros","userId":"10282691994518148206"}},"outputId":"8a755b70-4704-41e7-892f-dc1dc00124c7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["El tamaño total de GPT2 sin alteraciones es: 497759232 bytes o 474.70 MB\n"]}]},{"cell_type":"code","source":["tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","context = torch.tensor([tokenizer.encode(\"The planet earth is a beautiful\")])"],"metadata":{"id":"voxmoWW3go6l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(context, ntok=550):\n","    start_time = time.time()\n","    for _ in range(ntok):\n","        out = model(context)\n","        logits = out[:, -1, :]\n","        indices_to_remove = logits < torch.topk(logits, 10)[0][..., -1, None]\n","        logits[indices_to_remove] = -np.inf\n","        next_tok = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1).squeeze(1)\n","        context = torch.cat([context, next_tok.unsqueeze(-1)], dim=-1)\n","    end_time = time.time()\n","    inference_time = end_time - start_time\n","    return context, inference_time"],"metadata":{"id":"StTNnLCbg1xM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["out, inference_time = generate(context, ntok=40)\n","decoded_output = tokenizer.decode(out[0])"],"metadata":{"id":"1JzzXApkg_oP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Inference Time: {inference_time:.4f} seconds\")\n","print(f\"Generated Output: {decoded_output}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_LUY2n9IiBS8","executionInfo":{"status":"ok","timestamp":1738366816323,"user_tz":300,"elapsed":368,"user":{"displayName":"Jhenner Tigreros","userId":"10282691994518148206"}},"outputId":"b508de58-3ae8-49ee-b0b8-7f9e74bd220b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inference Time: 7.6916 seconds\n","Generated Output: The planet earth is a beautiful place that.\n","\n","\n","\n","\n","\n","\n","\n","( ) a: a) \" is a planet a \" a. a planet, a a. a. place the\n","\n",": a:\n"]}]}]}